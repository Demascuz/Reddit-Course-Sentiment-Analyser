{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links\n",
    "\n",
    "more comments error - https://www.reddit.com/r/redditdev/comments/5kwxb3/morecomments_object_attribute_error/ \n",
    "ML sentiment analysis - https://www.datarobot.com/blog/using-machine-learning-for-sentiment-analysis-a-deep-dive/#:~:text=Convolutional%20neural%20networks,used%20in%20computer%20vision%20models. \n",
    "Simple VADER sent analysis - https://medium.com/bitgrit-data-science-publication/sentiment-analysis-on-reddit-tech-news-with-python-cbaddb8e9bb6 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# misc\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "\n",
    "# reddit crawler\n",
    "import praw\n",
    "\n",
    "# sentiment analysis\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from nltk.tokenize import word_tokenize, RegexpTokenizer # tokenize words\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# nltk.download('vader_lexicon') # get lexicons data\n",
    "# nltk.download('punkt') # for tokenizer\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "#BERT sentiment analysis\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "r = praw.Reddit(user_agent='DemApollus',\n",
    "                client_id='KrnUePjuO_J50XuOWNIyRw',\n",
    "                client_secret='OswzYFW1nBcQWSfZPQVHmOXRglUcvw',\n",
    "                check_for_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nsid = SentimentIntensityAnalyzer()\\n\\npos_text = \"please take this class if u dont want to miss out on death!\"\\ncap_pos_text = \"Vader is AWESOME!\" # captilization and ! increases the effect\\nneg_text = \"Vader is bad\"\\n\\nprint(sid.polarity_scores(pos_text))\\nprint(sid.polarity_scores(cap_pos_text))\\nprint(sid.polarity_scores(neg_text)) \\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "pos_text = \"please take this class if u dont want to miss out on death!\"\n",
    "cap_pos_text = \"Vader is AWESOME!\" # captilization and ! increases the effect\n",
    "neg_text = \"Vader is bad\"\n",
    "\n",
    "print(sid.polarity_scores(pos_text))\n",
    "print(sid.polarity_scores(cap_pos_text))\n",
    "print(sid.polarity_scores(neg_text)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_code = \"CPSC 121\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubc_reddit = r.subreddit(\"UBC\")\n",
    "titles = list(())\n",
    "urls = list(())\n",
    "comments = list(())\n",
    "for post in ubc_reddit.search(course_code, limit=20):\n",
    "    titles.append(post.title)\n",
    "    urls.append(post.url)\n",
    "    submission = r.submission(id=post.id)\n",
    "    for top_level_comment in submission.comments:\n",
    "        comm = top_level_comment.body\n",
    "        if (len(comm) > 10 and len(comm) < 150):\n",
    "            comments.append(comm)\n",
    "    #for comment in submission.comments.list():\n",
    "        #print(post.title)\n",
    "        #comments.append(comment.body)\n",
    "        #print(comm)\n",
    "#print(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform sentiment analysis using BERT\n",
    "BSA_comments = list(())\n",
    "for i in comments:\n",
    "    tokens = tokenizer.encode(i, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    result.logits\n",
    "    BSA_comments.append(int(torch.argmax(result.logits))+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for j in range(len(BSA_comments)):\n",
    "    #print(str(BSA_comments[j]) + \": \" + comments[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Comments\": comments, \"BERT Ranking\": BSA_comments})\n",
    "df.to_csv(course_code + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
